{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FALejmVkha-"
      },
      "source": [
        "# Assignment 6: Sentiment Analysis Using LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkOMOOXAkhbB"
      },
      "source": [
        "In this assignment you will use an RNN/LSTM model for sentiment Analysis. You will use the movie reviews from the UCI Sentiment Labelled Sentences Dataset: https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SFOiguBPkhbB"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as K\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.models import *\n",
        "from tensorflow.keras.preprocessing import sequence\n",
        "from tensorflow.keras.datasets import imdb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2dY_ZlzkhbB"
      },
      "source": [
        "### Load the data\n",
        "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgKzen-jkhbC",
        "outputId": "d4cf2432-d337-4de9-9c2c-83aa474a304d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n",
            "25000 train sequences\n",
            "25000 test sequences\n"
          ]
        }
      ],
      "source": [
        "max_features = 20000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)\n",
        "print(len(x_train), 'train sequences')\n",
        "print(len(x_test), 'test sequences')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print pre processed review\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LF0EtbMxuT9m",
        "outputId": "ab61d88d-d3a3-4717-9ef2-c72814068b01"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[   15   256     4     2     7  3766     5   723    36    71    43   530\n",
            "   476    26   400   317    46     7     4 12118  1029    13   104    88\n",
            "     4   381    15   297    98    32  2071    56    26   141     6   194\n",
            "  7486    18     4   226    22    21   134   476    26   480     5   144\n",
            "    30  5535    18    51    36    28   224    92    25   104     4   226\n",
            "    65    16    38  1334    88    12    16   283     5    16  4472   113\n",
            "   103    32    15    16  5345    19   178    32]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COFDQUmkukON",
        "outputId": "d206cd70-c1ea-432e-a138-b91c57d7eb5a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qNGTxkPkhbC"
      },
      "source": [
        "#### Pad the sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "kWY_gUs9khbC"
      },
      "outputs": [],
      "source": [
        "maxlen = 80 \n",
        "batch_size = 32\n",
        "x_train = sequence.pad_sequences(x_train, maxlen=maxlen)\n",
        "x_test = sequence.pad_sequences(x_test, maxlen=maxlen)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpxyXLr3khbC"
      },
      "source": [
        "### Define the model\n",
        "define an LSTM model with an emebeding layer followed(to learn the embeddings), followed by 128 layers of LSTM units followed by aflatten, followed by two dense fully connceted layers 200 and 100 units, and finally a sigmoid layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "fDcGNJv2khbC"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(max_features, 128))\n",
        "model.add(LSTM(units=128, return_sequences=False))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(units=200, activation='relu'))\n",
        "model.add(Dense(units=100, activation='relu'))\n",
        "model.add(Dense(units=1, activation='sigmoid'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5wSDJqMOkhbC"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vCRFXdmkhbD",
        "outputId": "e3d96d43-ff4a-4573-fb28-f4cf6774cb74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 212s 266ms/step - loss: 0.1493 - accuracy: 0.9434 - val_loss: 0.4765 - val_accuracy: 0.8264\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 210s 268ms/step - loss: 0.0806 - accuracy: 0.9712 - val_loss: 0.6114 - val_accuracy: 0.8181\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 212s 271ms/step - loss: 0.0537 - accuracy: 0.9808 - val_loss: 0.8023 - val_accuracy: 0.8102\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 212s 271ms/step - loss: 0.0359 - accuracy: 0.9880 - val_loss: 0.6731 - val_accuracy: 0.8033\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 213s 273ms/step - loss: 0.0294 - accuracy: 0.9906 - val_loss: 1.1217 - val_accuracy: 0.8134\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fe2a629e650>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "model.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=5,\n",
        "          validation_data=(x_test, y_test))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfoZfZc3khbD",
        "outputId": "34688adb-b204-4e9e-dea3-a6db11cd3d65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "782/782 [==============================] - 51s 65ms/step - loss: 1.1217 - accuracy: 0.8134\n",
            "Test score: 1.121737003326416\n",
            "Test accuracy: 0.8133999705314636\n"
          ]
        }
      ],
      "source": [
        "score, acc = model.evaluate(x_test, y_test,\n",
        "                            batch_size=batch_size)\n",
        "print('Test score:', score)\n",
        "print('Test accuracy:', acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "epoch was intentionally set to 5 to reduce run time. accuracy is slighly compromised"
      ],
      "metadata": {
        "id": "pefnSMKLtzrS"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}